{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import math\n",
    "# sys.path = [sys.path[-1]] + sys.path[:-1] # move conda env to beginning of sys.path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import tensorrt\n",
    "\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append('/home/nikita/e2e-driving')\n",
    "\n",
    "from dataloading.nvidia import NvidiaValidationDataset\n",
    "from metrics.metrics import calculate_open_loop_metrics\n",
    "from _tensorrt.tensorrt_util import TensorrtModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, engine='ort', channel_order=[0,1,2]):\n",
    "    all_predictions = []\n",
    "    inference_times = []\n",
    "    progress_bar = tqdm(total=len(dataloader), smoothing=0)\n",
    "    inputs_name = model.get_inputs()[0].name if 'ort' in engine  else None\n",
    "    batch_size = dataloader.batch_size\n",
    "\n",
    "    epoch_mae = 0.0\n",
    "    ask_batch_timestamp = time.time()\n",
    "    for i, (input, target, _) in enumerate(dataloader):\n",
    "\n",
    "        if input['image'].shape[0] != batch_size:\n",
    "            print(f'Dropping batch with {input[\"image\"].shape[0]} samples')\n",
    "            continue\n",
    "\n",
    "        # if i < 3400:\n",
    "        #     all_predictions.extend(np.array([0]).repeat(batch_size))\n",
    "        #     progress_bar.update(1)\n",
    "        #     continue\n",
    "\n",
    "        # inputs = input['image'].cpu().numpy()[:, channel_order, ...]\n",
    "        inputs = input['image'].cpu().numpy()[:, channel_order, ...]\n",
    "\n",
    "        # cv2 convert rgb to bgr\n",
    "\n",
    "        # print(type(inputs), inputs.shape, inputs.dtype, inputs.min(), inputs.max(), inputs.mean())\n",
    "\n",
    "        trt_inputs = np.ascontiguousarray(inputs)\n",
    "        ort_inputs = {inputs_name: inputs}\n",
    "\n",
    "        inference_start = time.perf_counter()\n",
    "        if 'trt' in engine:\n",
    "            preds = model.predict(trt_inputs)\n",
    "        elif 'ort' in engine:\n",
    "            preds = model.run(None, ort_inputs)[0]\n",
    "            preds = preds.squeeze().reshape(-1, 1)\n",
    "        inference_end = time.perf_counter()\n",
    "\n",
    "        # print('preds:', preds)\n",
    "\n",
    "        inference_time = inference_end - inference_start\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        mae = F.l1_loss(torch.as_tensor(preds).reshape(-1,1), target.view(-1, 1))\n",
    "        mae_degrees = math.degrees(mae.item())\n",
    "        epoch_mae += mae_degrees\n",
    "\n",
    "        if 'ort' in engine:\n",
    "            all_predictions.extend(preds)\n",
    "        elif 'trt' in engine:\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_description = f'MAE: {(epoch_mae / (i + 1)):.4f}'\n",
    "        if 'trt' in engine:\n",
    "            progress_description += f', Preds: {preds:.4f}'\n",
    "        progress_bar.set_description(progress_description)\n",
    "\n",
    "\n",
    "    avg_mae = epoch_mae / len(dataloader)\n",
    "    result = np.array(all_predictions)\n",
    "    return avg_mae, result\n",
    "\n",
    "\n",
    "def calculate_metrics(fps, predictions, valid_loader):\n",
    "    '''For steering angle only.'''\n",
    "\n",
    "    frames_df = valid_loader.dataset.frames.copy()\n",
    "\n",
    "    # support dropping smaller batches\n",
    "    batch_size = valid_loader.batch_size\n",
    "    n_drop = frames_df.shape[0] % batch_size\n",
    "    if n_drop > 0:\n",
    "        frames_df = frames_df[:-n_drop]\n",
    "\n",
    "    true_steering_angles = frames_df.steering_angle.to_numpy()\n",
    "    metrics = calculate_open_loop_metrics(predictions, true_steering_angles, fps=fps)\n",
    "\n",
    "    left_turns = frames_df[\"turn_signal\"] == 0  # TODO: remove magic values\n",
    "    left_metrics = calculate_open_loop_metrics(predictions[left_turns], true_steering_angles[left_turns], fps=fps)\n",
    "    metrics[\"left_mae\"] = left_metrics[\"mae\"]\n",
    "\n",
    "    straight = frames_df[\"turn_signal\"] == 1\n",
    "    straight_metrics = calculate_open_loop_metrics(predictions[straight], true_steering_angles[straight], fps=fps)\n",
    "    metrics[\"straight_mae\"] = straight_metrics[\"mae\"]\n",
    "\n",
    "    right_turns = frames_df[\"turn_signal\"] == 2\n",
    "    right_metrics = calculate_open_loop_metrics(predictions[right_turns], true_steering_angles[right_turns], fps=fps)\n",
    "    metrics[\"right_mae\"] = right_metrics[\"mae\"]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def convert_static_to_dynamic_bs(onnx_path_in, onnx_path_out):\n",
    "    model = onnx.load(onnx_path_in)\n",
    "    for inputs in model.graph.input:\n",
    "        dim1 = inputs.type.tensor_type.shape.dim[0]\n",
    "        dim1.dim_param = 'batch'\n",
    "    onnx.save(model, onnx_path_out)\n",
    "\n",
    "def convert_dynamic_to_static_bs(onnx_path_in, onnx_path_out):\n",
    "    model = onnx.load(onnx_path_in)\n",
    "    for inputs in model.graph.input:\n",
    "        dim1 = inputs.type.tensor_type.shape.dim[0]\n",
    "        dim1.dim_value = 1\n",
    "    onnx.save(model, onnx_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Bolt/dataset-new-small/summer2021/2021-05-28-15-19-48_e2e_sulaoja_20_30: lenght=10708, filtered=0\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-06-07-14-20-07_e2e_rec_ss6: lenght=25836, filtered=1\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-06-07-14-06-31_e2e_rec_ss6: lenght=3003, filtered=0\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-06-07-14-09-18_e2e_rec_ss6: lenght=4551, filtered=1\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-06-07-14-36-16_e2e_rec_ss6: lenght=25368, filtered=1\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-09-24-14-03-45_e2e_rec_ss11_backwards: lenght=25172, filtered=0\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-10-26-10-49-06_e2e_rec_ss20_elva: lenght=33045, filtered=0\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-10-26-11-08-59_e2e_rec_ss20_elva_back: lenght=33281, filtered=0\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-10-20-15-11-29_e2e_rec_vastse_ss13_17_back: lenght=26763, filtered=0\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-10-11-14-50-59_e2e_rec_vahi: lenght=21211, filtered=2\n",
      "/data/Bolt/dataset-new-small/summer2021/2021-10-14-13-08-51_e2e_rec_vahi_backwards: lenght=13442, filtered=0\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/data/Bolt/dataset-new-small/summer2021'\n",
    "output_modality = 'steering_angle'\n",
    "n_branches = 1\n",
    "n_waypoints = 1\n",
    "batch_size = 1\n",
    "num_workers = 8\n",
    "\n",
    "validset = NvidiaValidationDataset(Path(dataset_path), output_modality, n_branches, n_waypoints=1)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False,\n",
    "                                          num_workers=num_workers, pin_memory=True,\n",
    "                                          persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla_pilotnet_path = '/home/nikita/e2e-driving/_models/vanilla-pilotnet.onnx'\n",
    "# autumn_v3_path = '/home/nikita/e2e-driving/_models/autumn-v3.onnx'\n",
    "# autumn_v3_ort_path = '/home/nikita/e2e-driving/_models/dynamic_autumn-v3.onnx'\n",
    "\n",
    "\n",
    "# model_paths = ['/home/nikita/e2e-driving/_models/dynamic_autumn-v3.onnx']\n",
    "# model_paths = [models_dir / 'ibc-dfo-3-1024-static.onnx']\n",
    "# model_paths = [models_dir / '.onnx']\n",
    "model_paths = ['/home/nikita/e2e-driving/_models/vanilla-pilotnet-static.onnx']\n",
    "models = [TensorrtModel(model_path) for model_path in model_paths]\n",
    "model_names = ['vanilla-pilotnet']\n",
    "model_runtimes = [f'trt{tensorrt.__version__}']\n",
    "\n",
    "# vanilla_pilotnet = ort.InferenceSession(vanilla_pilotnet_path, providers=['CUDAExecutionProvider'])\n",
    "# autumn_v3 = ort.InferenceSession(autumn_v3_path, providers=['CUDAExecutionProvider'])\n",
    "\n",
    "# autumn_v3_trt = TensorrtModel(autumn_v3_path)\n",
    "# autumn_v3_ort = ort.InferenceSession(autumn_v3_ort_path, providers=['CUDAExecutionProvider'])\n",
    "\n",
    "# model = TensorrtModel('/home/nikita/e2e-driving/_models/autumn-v3.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw inputs: torch.Size([256, 3, 68, 264])\n",
      "onnx inputs batched: (256, 3, 68, 264)\n",
      "onnx inputs single: (1, 3, 68, 264)\n",
      "inputs are equal\n",
      "dynamic_batch_outs: (1,)\n",
      "dynamic_single_outs: (1,)\n",
      "outs are equal\n"
     ]
    }
   ],
   "source": [
    "# verify that dynamic model & bs1_model give same results when fed with batch vs single image\n",
    "\n",
    "dynamic_model = models[0]\n",
    "\n",
    "inputs, _, __ = iter(valid_loader).next()\n",
    "print('raw inputs:', inputs['image'].shape)\n",
    "\n",
    "batched_input = {dynamic_model.get_inputs()[0].name: inputs['image'].cpu().numpy()}\n",
    "single_input = {dynamic_model.get_inputs()[0].name: inputs['image'].cpu().numpy()[0, None, ...]}\n",
    "\n",
    "batch_input_np =  batched_input[list(batched_input.keys())[0]]\n",
    "single_input_np = single_input[list(single_input.keys())[0]]\n",
    "\n",
    "print('onnx inputs batched:', batch_input_np.shape)\n",
    "print('onnx inputs single:', single_input_np.shape)\n",
    "\n",
    "np.testing.assert_equal(batch_input_np[0], single_input_np[0])\n",
    "print('inputs are equal')\n",
    "\n",
    "dynamic_batched_outs = dynamic_model.run(None, batched_input)[0][0]\n",
    "dynamic_single_outs = dynamic_model.run(None, single_input)[0][0]\n",
    "# bs1_single_outs = bs1_model.run(None, single_input)[0][0]\n",
    "\n",
    "print('dynamic_batch_outs:', dynamic_batched_outs.shape)\n",
    "print('dynamic_single_outs:', dynamic_single_outs.shape)\n",
    "# print('bs1_single_outs:', bs1_single_outs.shape)\n",
    "\n",
    "np.testing.assert_almost_equal(dynamic_batched_outs, dynamic_single_outs)\n",
    "# np.testing.assert_almost_equal(bs1_single_outs, dynamic_single_outs)\n",
    "print('outs are equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416 µs ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "dynamic_single_outs = dynamic_model.run(None, single_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10g6a41x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4471387092134061b66a911188f87dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.054 MB of 0.054 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vanilla-pilotnet</strong>: <a href=\"https://wandb.ai/nikebless-thesis/ibc-tuning/runs/10g6a41x\" target=\"_blank\">https://wandb.ai/nikebless-thesis/ibc-tuning/runs/10g6a41x</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220708_111401-10g6a41x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10g6a41x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikita/e2e-driving/notebooks/wandb/run-20220708_111829-17fuxxtl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nikebless-thesis/ibc-tuning/runs/17fuxxtl\" target=\"_blank\">vanilla-pilotnet</a></strong> to <a href=\"https://wandb.ai/nikebless-thesis/ibc-tuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model vanilla-pilotnet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74001a1c432d48d5bba0700c6dd078f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment metrics:\n",
      "\"{'mae': 8.460469773816776, 'rmse': 28.71684608129731, 'max': 774.800240904782, 'whiteness': 166.29128, 'expert_whiteness': 25.61257234059799, 'left_mae': 42.27578282599571, 'straight_mae': 5.821816372862548, 'right_mae': 68.15964181056624, '_timestamp': 1657270421, '_runtime': 2103}\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e96d4650b443b1844055e6d33d00b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.054 MB of 0.054 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>expert_whiteness</td><td>▁</td></tr><tr><td>left_mae</td><td>▁</td></tr><tr><td>mae</td><td>▁</td></tr><tr><td>max</td><td>▁</td></tr><tr><td>right_mae</td><td>▁</td></tr><tr><td>rmse</td><td>▁</td></tr><tr><td>straight_mae</td><td>▁</td></tr><tr><td>whiteness</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>expert_whiteness</td><td>25.61257</td></tr><tr><td>left_mae</td><td>42.27578</td></tr><tr><td>mae</td><td>8.46047</td></tr><tr><td>max</td><td>774.80024</td></tr><tr><td>right_mae</td><td>68.15964</td></tr><tr><td>rmse</td><td>28.71685</td></tr><tr><td>straight_mae</td><td>5.82182</td></tr><tr><td>whiteness</td><td>166.29128</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vanilla-pilotnet</strong>: <a href=\"https://wandb.ai/nikebless-thesis/ibc-tuning/runs/17fuxxtl\" target=\"_blank\">https://wandb.ai/nikebless-thesis/ibc-tuning/runs/17fuxxtl</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220708_111829-17fuxxtl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "fps = 30\n",
    "channel_order = [0, 1, 2]\n",
    "# channel_orders = [[0,1,2], [0,2,1], [1,0,2], [1,2,0], [2,0,1], [2,1,0]]\n",
    "\n",
    "\n",
    "# for channel_order in channel_orders:\n",
    "for model, model_name, model_path, inference_eng in zip(models, model_names, model_paths, model_runtimes):\n",
    "    wandb.init(project=\"ibc-tuning\", name=model_name, config={\"model_path\": model_path, \"channel_order\": channel_order}, tags=[inference_eng])\n",
    "\n",
    "    print(f'Evaluating model {model_name}...')\n",
    "    mae, preds = evaluate(model, valid_loader, inference_eng, channel_order)\n",
    "    metrics = calculate_metrics(fps, preds.squeeze(), valid_loader)\n",
    "    wandb.log(metrics)\n",
    "    print('experiment metrics:')\n",
    "    print(json.dumps(str(metrics), indent=2))\n",
    "    print()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before surgery: [4.1178102]\n",
      "after surgery: [[4.1178093]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.randn(1, 3, 68, 264).astype(np.float32)\n",
    "surgery_out_path = '/home/nikita/e2e-driving/_models/autumn-v3-bs1.onnx'\n",
    "\n",
    "# ORT before surgery\n",
    "autumn_v3_ort = ort.InferenceSession(autumn_v3_path, providers=['CUDAExecutionProvider'])\n",
    "outs = autumn_v3_ort.run(None, {'input.1': inputs.repeat(64, axis=0)})[0][0]\n",
    "print('before surgery:', outs)\n",
    "\n",
    "# surgery\n",
    "autumn_v3_onnx = onnx.load(autumn_v3_path)\n",
    "autumn_v3_onnx.graph.input[0].type.tensor_type.shape.dim[0].dim_value = 1\n",
    "onnx.save_model(autumn_v3_onnx, surgery_out_path)\n",
    "\n",
    "# ORT after surgery\n",
    "autumn_v3_b1_ort = ort.InferenceSession(surgery_out_path, providers=['CUDAExecutionProvider'])\n",
    "outs = autumn_v3_b1_ort.run(None, {'input.1': inputs})[0]\n",
    "print('after surgery:', outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_static_to_dynamic_bs('/home/nikita/e2e-driving/_models/autumn-v3.onnx', '/home/nikita/e2e-driving/_models/autumn-v3-dynamic.onnx')\n",
    "models_dir = Path('/home/nikita/ros-e2e-workspace/src/e2e_platform/config/nvidia_e2e_models/')\n",
    "\n",
    "convert_dynamic_to_static_bs('/home/nikita/e2e-driving/_models/vanilla-pilotnet.onnx', '/home/nikita/e2e-driving/_models/vanilla-pilotnet-static.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('e2e38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ce61fbaff1d6f64d0e241387617123ad497002f7cadff1725fb90af458228dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
