{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss, LogSoftmax, KLDivLoss, MSELoss, L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: tensor(0.0617)\n",
      "L1: tensor(0.2750)\n",
      "L2: tensor(0.1175)\n",
      "EMD: tensor(5612170.)\n"
     ]
    }
   ],
   "source": [
    "# cel = CrossEntropyLoss()\n",
    "kll = KLDivLoss(reduction='batchmean')\n",
    "msl = MSELoss()\n",
    "l1l = L1Loss()\n",
    "\n",
    "def earth_mover_distance(y_true, y_pred):\n",
    "    return torch.square(torch.cumsum(y_true, dim=-1) - torch.cumsum(y_pred, dim=-1)).sum() / y_true.size(0)\n",
    "\n",
    "logSoftmax = LogSoftmax(dim=1)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "preds = torch.tensor([[0.1, 0.1, 0.3, 0.7]]).repeat(512, 1024//4)\n",
    "targets = torch.tensor([[0.2, 0.2, 0.9, 0.4]]).repeat(512, 1024//4)\n",
    "targets[0, 0] += 0.1\n",
    "targets[0, 1023] += 0.1\n",
    "\n",
    "\n",
    "# print('CE:', cel(preds, softmax(targets)))\n",
    "print('KLD:', kll(logSoftmax(preds), softmax(targets)))\n",
    "print('L1:', l1l(preds, targets))\n",
    "print('L2:', msl(preds, targets))\n",
    "print('EMD:', earth_mover_distance(targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(507.8565)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.randn(512, 512)\n",
    "targets = torch.randn(512, 512)\n",
    "\n",
    "torch.mean(earth_mover_distance(preds, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1623e-07)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kll(F.log_softmax(preds, dim=1), F.softmax(targets, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "         0.1000, 0.2000, 0.3000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0.1, 0.2, 0.3]]).repeat(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kll(logSoftmax(preds), softmax(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0953)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cel(logSoftmax(preds), softmax(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(172)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 55, 172,  10, 106, 260, 325,   5, 281,   3, 511,  63,  27, 413, 333,\n",
       "        456, 264, 399, 209, 454, 179, 108, 365, 249, 150,  84,  17, 450,  42,\n",
       "        163,   8, 446, 270, 484, 509, 479, 226, 486, 445, 172, 463, 140, 277,\n",
       "        267,  43, 153, 386, 168,  93, 402,  26, 333, 403,  74, 307, 415, 497,\n",
       "         69, 250, 172,  76, 322, 170, 490, 353, 271,   5, 504, 511, 300, 185,\n",
       "        158,  41, 458, 409, 164, 115, 403, 279,  79, 404, 258, 367,  40, 126,\n",
       "        370, 227, 270, 445, 326, 402, 184,  81, 490, 321, 359, 285, 476,  89,\n",
       "        450,  42,  34,  25,  94, 406, 268, 394, 492, 480, 398, 176, 498, 258,\n",
       "         26, 237, 508,  18, 207, 496, 497,  89, 458, 341,  85, 258, 161, 166,\n",
       "        339,  70, 275, 458, 270, 437, 172, 268, 318, 425, 363, 182, 109, 285,\n",
       "        399, 140, 396, 407, 136, 161, 238, 308, 377,  58, 461, 379,   5, 504,\n",
       "         29, 141, 328,  85, 201, 141,  30, 130, 255, 411, 253, 253, 182, 430,\n",
       "        463,  63, 184, 116, 205, 142, 198, 475, 417, 361, 121, 491, 423, 269,\n",
       "         67, 419, 105, 119, 105, 206, 171, 375, 231,  76, 196, 413, 146, 317,\n",
       "        373, 312, 255, 504, 498,  21, 161, 203, 191, 373, 145, 375,  29, 445,\n",
       "        304, 280, 161,  45, 380, 222, 258, 387, 431, 112, 476, 361, 353,  41,\n",
       "        231, 110, 220, 206, 401, 191, 425, 259, 200,   6, 392,  61, 460,  71,\n",
       "        153, 290, 218, 153, 434,  36, 455, 156,  62, 135, 113, 504, 470, 346,\n",
       "        103, 112, 267, 318, 285, 484, 260, 285, 483, 209, 277, 188, 368, 474,\n",
       "        486, 504, 284, 397, 457, 250,  13, 408, 413, 101,  32, 383,  10,  79,\n",
       "        410,  31, 189, 146, 263, 300, 161, 289, 459, 430, 188, 444,  39, 333,\n",
       "        143,  93, 184, 431, 491,   4, 125, 387, 252, 269,  90, 469, 137, 208,\n",
       "        450, 228, 160, 500,  91,  27, 319,  73, 430, 178, 508, 113, 170,  51,\n",
       "        348, 292,  96,  48, 265, 351, 485, 463, 272, 373, 377, 279, 294, 112,\n",
       "        322, 253,  83, 257, 480, 254, 107, 124,  13, 262, 174, 194, 227,  76,\n",
       "        248, 310, 455, 233, 157, 479, 441, 296, 367, 269, 199,  69, 268,  90,\n",
       "        186, 377, 372,  88, 441, 338,  52, 153, 404,  12, 261, 250,  46, 248,\n",
       "         77, 462, 314, 340, 364, 309, 339, 155, 353, 108, 260, 472, 292,  62,\n",
       "         19, 416, 320, 255, 220,  87,  49, 394, 451, 269, 493, 326, 130, 264,\n",
       "        496, 155, 105, 367,   9, 509, 355,  50, 243, 195, 242, 492,  55,  46,\n",
       "        421,  24, 249, 399, 397, 369, 328, 418, 137, 160, 263, 195,  31, 269,\n",
       "        497, 331, 221, 347, 440,  94, 135, 423,  44, 408, 504, 204, 259,  92,\n",
       "        123, 354, 309,  12, 426, 196, 283,  32, 190, 342,  31, 302, 159, 175,\n",
       "        353, 303, 442, 369,  35, 476, 365, 299, 480, 424, 398, 314,  76, 298,\n",
       "        511, 138, 374,  81, 288, 280, 384, 450, 133, 215,  12,  23,  64, 149,\n",
       "        337, 302,  87, 403, 164,  59, 396, 278, 290, 357, 286,  88, 501, 343,\n",
       "        155, 429, 161, 468,  86,  25, 168, 405])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ground_truth = torch.arange(logits.shape[1])\n",
    "non_ground_truth = non_ground_truth[non_ground_truth != ground_truth]\n",
    "non_gt_logits = logits[:, non_ground_truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)\n",
    "value = 5\n",
    "x = x[x!=value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x!=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 513]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "targets = torch.linspace(-1, 1, 513).repeat(512, 1)\n",
    "print(targets.shape, targets.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 513]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "permutation = torch.rand(targets.size(0), targets.size(1)).argsort(dim=1)\n",
    "print(permutation.shape, permutation.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 513]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "targets = targets[torch.arange(targets.size(0)).unsqueeze(-1), permutation]\n",
    "print(targets.shape, targets.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 513]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "unscrambled_targets = targets[torch.arange(targets.size(0)).unsqueeze(-1), torch.argsort(permutation)]\n",
    "print(unscrambled_targets.shape, unscrambled_targets.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -0.9961, -0.9922, -0.9883, -0.9844, -0.9805, -0.9766, -0.9727,\n",
       "        -0.9688, -0.9648, -0.9609, -0.9570, -0.9531, -0.9492, -0.9453, -0.9414,\n",
       "        -0.9375, -0.9336, -0.9297, -0.9258, -0.9219, -0.9180, -0.9141, -0.9102,\n",
       "        -0.9062, -0.9023, -0.8984, -0.8945, -0.8906, -0.8867, -0.8828, -0.8789,\n",
       "        -0.8750, -0.8711, -0.8672, -0.8633, -0.8594, -0.8555, -0.8516, -0.8477,\n",
       "        -0.8438, -0.8398, -0.8359, -0.8320, -0.8281, -0.8242, -0.8203, -0.8164,\n",
       "        -0.8125, -0.8086, -0.8047, -0.8008, -0.7969, -0.7930, -0.7891, -0.7852,\n",
       "        -0.7812, -0.7773, -0.7734, -0.7695, -0.7656, -0.7617, -0.7578, -0.7539,\n",
       "        -0.7500, -0.7461, -0.7422, -0.7383, -0.7344, -0.7305, -0.7266, -0.7227,\n",
       "        -0.7188, -0.7148, -0.7109, -0.7070, -0.7031, -0.6992, -0.6953, -0.6914,\n",
       "        -0.6875, -0.6836, -0.6797, -0.6758, -0.6719, -0.6680, -0.6641, -0.6602,\n",
       "        -0.6562, -0.6523, -0.6484, -0.6445, -0.6406, -0.6367, -0.6328, -0.6289,\n",
       "        -0.6250, -0.6211, -0.6172, -0.6133, -0.6094, -0.6055, -0.6016, -0.5977,\n",
       "        -0.5938, -0.5898, -0.5859, -0.5820, -0.5781, -0.5742, -0.5703, -0.5664,\n",
       "        -0.5625, -0.5586, -0.5547, -0.5508, -0.5469, -0.5430, -0.5391, -0.5352,\n",
       "        -0.5312, -0.5273, -0.5234, -0.5195, -0.5156, -0.5117, -0.5078, -0.5039,\n",
       "        -0.5000, -0.4961, -0.4922, -0.4883, -0.4844, -0.4805, -0.4766, -0.4727,\n",
       "        -0.4688, -0.4648, -0.4609, -0.4570, -0.4531, -0.4492, -0.4453, -0.4414,\n",
       "        -0.4375, -0.4336, -0.4297, -0.4258, -0.4219, -0.4180, -0.4141, -0.4102,\n",
       "        -0.4062, -0.4023, -0.3984, -0.3945, -0.3906, -0.3867, -0.3828, -0.3789,\n",
       "        -0.3750, -0.3711, -0.3672, -0.3633, -0.3594, -0.3555, -0.3516, -0.3477,\n",
       "        -0.3438, -0.3398, -0.3359, -0.3320, -0.3281, -0.3242, -0.3203, -0.3164,\n",
       "        -0.3125, -0.3086, -0.3047, -0.3008, -0.2969, -0.2930, -0.2891, -0.2852,\n",
       "        -0.2812, -0.2773, -0.2734, -0.2695, -0.2656, -0.2617, -0.2578, -0.2539,\n",
       "        -0.2500, -0.2461, -0.2422, -0.2383, -0.2344, -0.2305, -0.2266, -0.2227,\n",
       "        -0.2188, -0.2148, -0.2109, -0.2070, -0.2031, -0.1992, -0.1953, -0.1914,\n",
       "        -0.1875, -0.1836, -0.1797, -0.1758, -0.1719, -0.1680, -0.1641, -0.1602,\n",
       "        -0.1562, -0.1523, -0.1484, -0.1445, -0.1406, -0.1367, -0.1328, -0.1289,\n",
       "        -0.1250, -0.1211, -0.1172, -0.1133, -0.1094, -0.1055, -0.1016, -0.0977,\n",
       "        -0.0938, -0.0898, -0.0859, -0.0820, -0.0781, -0.0742, -0.0703, -0.0664,\n",
       "        -0.0625, -0.0586, -0.0547, -0.0508, -0.0469, -0.0430, -0.0391, -0.0352,\n",
       "        -0.0312, -0.0273, -0.0234, -0.0195, -0.0156, -0.0117, -0.0078, -0.0039,\n",
       "         0.0000,  0.0039,  0.0078,  0.0117,  0.0156,  0.0195,  0.0234,  0.0273,\n",
       "         0.0312,  0.0352,  0.0391,  0.0430,  0.0469,  0.0508,  0.0547,  0.0586,\n",
       "         0.0625,  0.0664,  0.0703,  0.0742,  0.0781,  0.0820,  0.0859,  0.0898,\n",
       "         0.0938,  0.0977,  0.1016,  0.1055,  0.1094,  0.1133,  0.1172,  0.1211,\n",
       "         0.1250,  0.1289,  0.1328,  0.1367,  0.1406,  0.1445,  0.1484,  0.1523,\n",
       "         0.1562,  0.1602,  0.1641,  0.1680,  0.1719,  0.1758,  0.1797,  0.1836,\n",
       "         0.1875,  0.1914,  0.1953,  0.1992,  0.2031,  0.2070,  0.2109,  0.2148,\n",
       "         0.2188,  0.2227,  0.2266,  0.2305,  0.2344,  0.2383,  0.2422,  0.2461,\n",
       "         0.2500,  0.2539,  0.2578,  0.2617,  0.2656,  0.2695,  0.2734,  0.2773,\n",
       "         0.2812,  0.2852,  0.2891,  0.2930,  0.2969,  0.3008,  0.3047,  0.3086,\n",
       "         0.3125,  0.3164,  0.3203,  0.3242,  0.3281,  0.3320,  0.3359,  0.3398,\n",
       "         0.3438,  0.3477,  0.3516,  0.3555,  0.3594,  0.3633,  0.3672,  0.3711,\n",
       "         0.3750,  0.3789,  0.3828,  0.3867,  0.3906,  0.3945,  0.3984,  0.4023,\n",
       "         0.4062,  0.4102,  0.4141,  0.4180,  0.4219,  0.4258,  0.4297,  0.4336,\n",
       "         0.4375,  0.4414,  0.4453,  0.4492,  0.4531,  0.4570,  0.4609,  0.4648,\n",
       "         0.4688,  0.4727,  0.4766,  0.4805,  0.4844,  0.4883,  0.4922,  0.4961,\n",
       "         0.5000,  0.5039,  0.5078,  0.5117,  0.5156,  0.5195,  0.5234,  0.5273,\n",
       "         0.5312,  0.5352,  0.5391,  0.5430,  0.5469,  0.5508,  0.5547,  0.5586,\n",
       "         0.5625,  0.5664,  0.5703,  0.5742,  0.5781,  0.5820,  0.5859,  0.5898,\n",
       "         0.5938,  0.5977,  0.6016,  0.6055,  0.6094,  0.6133,  0.6172,  0.6211,\n",
       "         0.6250,  0.6289,  0.6328,  0.6367,  0.6406,  0.6445,  0.6484,  0.6523,\n",
       "         0.6562,  0.6602,  0.6641,  0.6680,  0.6719,  0.6758,  0.6797,  0.6836,\n",
       "         0.6875,  0.6914,  0.6953,  0.6992,  0.7031,  0.7070,  0.7109,  0.7148,\n",
       "         0.7188,  0.7227,  0.7266,  0.7305,  0.7344,  0.7383,  0.7422,  0.7461,\n",
       "         0.7500,  0.7539,  0.7578,  0.7617,  0.7656,  0.7695,  0.7734,  0.7773,\n",
       "         0.7812,  0.7852,  0.7891,  0.7930,  0.7969,  0.8008,  0.8047,  0.8086,\n",
       "         0.8125,  0.8164,  0.8203,  0.8242,  0.8281,  0.8320,  0.8359,  0.8398,\n",
       "         0.8438,  0.8477,  0.8516,  0.8555,  0.8594,  0.8633,  0.8672,  0.8711,\n",
       "         0.8750,  0.8789,  0.8828,  0.8867,  0.8906,  0.8945,  0.8984,  0.9023,\n",
       "         0.9062,  0.9102,  0.9141,  0.9180,  0.9219,  0.9258,  0.9297,  0.9336,\n",
       "         0.9375,  0.9414,  0.9453,  0.9492,  0.9531,  0.9570,  0.9609,  0.9648,\n",
       "         0.9688,  0.9727,  0.9766,  0.9805,  0.9844,  0.9883,  0.9922,  0.9961,\n",
       "         1.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscrambled_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2344, -0.1953, -0.9883, -0.3711,  0.3477, -0.4062,  0.2188,  0.1094,\n",
       "         0.1680, -0.9375, -0.0742, -0.4375, -0.0664,  0.1562,  0.7266,  0.3164,\n",
       "         0.3789, -0.7930,  0.6367, -0.7852,  0.1523, -0.0352,  0.4844, -0.2812,\n",
       "         0.4180, -0.3047,  0.9375, -0.5039,  0.1250, -0.5273,  0.8359, -0.4492,\n",
       "        -0.8203,  0.7031,  0.3359,  0.7305,  0.1289, -0.9727,  0.0234,  0.4023,\n",
       "         0.8086,  0.9688,  0.0000,  0.6055,  0.0703, -0.0156,  0.4102, -0.5664,\n",
       "        -0.1992, -0.7656, -0.0859, -0.7969, -0.5078,  0.2578, -0.9609, -0.9531,\n",
       "        -0.0820, -0.3633, -0.0469,  0.8242,  0.7344, -0.8477,  0.7461,  0.0586,\n",
       "         0.6445, -0.8945, -0.3555, -0.6914,  0.7930, -0.6289,  0.0273, -0.9766,\n",
       "         0.3672,  0.5469, -0.7812,  0.2344,  0.8047,  0.7109,  0.4688,  0.3281,\n",
       "         0.6758, -0.7461,  0.8320, -0.8398, -0.8594, -0.8164,  0.5547, -0.9297,\n",
       "        -0.3320,  0.1719, -0.2617,  0.3320,  0.2773,  0.5273, -0.9922,  0.5664,\n",
       "        -0.4336,  0.8711,  0.9180, -0.1484,  0.9414,  0.0117, -0.5000, -0.7695,\n",
       "        -0.1641, -0.4102, -0.3672,  0.8750,  0.1992,  0.8516, -0.3789, -0.7305,\n",
       "         0.3242,  0.2539, -0.6641,  0.0195, -0.8438,  0.4648,  0.2695, -0.8750,\n",
       "         0.8633, -0.6836,  0.3555,  0.1758, -0.0312,  0.1875, -0.1445,  0.6836,\n",
       "         0.6484,  0.9961,  0.2031,  0.5312,  0.6523, -0.4766,  0.6602,  0.0859,\n",
       "        -0.6484,  1.0000,  0.8906,  0.7578,  0.3438, -0.0391,  0.7227, -0.9258,\n",
       "        -0.1289, -0.6055, -0.1250, -0.9414, -0.6875, -0.6680, -0.4844, -0.6367,\n",
       "        -0.4883,  0.2969,  0.6641, -0.2070,  0.8555, -0.0625, -0.7422,  0.2305,\n",
       "        -0.4023, -0.1836,  0.9805, -0.8555, -0.9648,  0.2734, -0.5625, -0.1719,\n",
       "         0.9922,  0.2852,  0.0156, -0.7031,  0.6875,  0.0508,  0.8281, -0.3164,\n",
       "        -0.9844,  0.4062, -0.9805,  0.5508,  0.5703, -0.8633,  0.7617, -0.9961,\n",
       "        -0.1797,  0.3086, -0.6758,  0.2891, -0.5312, -0.9219, -0.6562, -0.6445,\n",
       "         0.2266,  0.2656, -0.1758, -0.6602, -0.7148, -0.6328, -0.0117,  0.1328,\n",
       "         0.7070,  0.7812,  0.1406, -0.9336,  0.6133, -0.5195,  0.6250, -0.3984,\n",
       "         0.7383, -0.3906, -0.2695,  0.3398,  0.8125, -0.0938, -0.2891,  0.9453,\n",
       "        -0.2773, -0.6172,  0.2930,  0.2227,  0.5391, -0.5703, -0.7773,  0.0742,\n",
       "         0.9336, -0.2852, -0.8281, -0.9688, -0.1914,  0.5234, -0.3945, -0.9102,\n",
       "         0.5195, -0.5469,  0.1367, -0.7344,  0.3125, -0.3359,  0.5430,  0.8203,\n",
       "         0.5977, -0.0430,  0.9062,  0.0078,  0.9297, -0.3477,  0.0039,  0.7656,\n",
       "         0.1172,  0.4766, -0.6953, -0.7500, -0.3086, -0.5938, -0.4648, -0.4609,\n",
       "         0.0469,  0.8945, -0.2148, -0.1680, -0.4531,  0.1836, -0.2422, -0.5977,\n",
       "         0.2383,  0.5781, -0.8047,  0.0781, -0.4727,  0.2500,  0.8477, -0.3516,\n",
       "         0.0430, -0.4453,  0.6992,  0.7188, -0.6211,  0.7773,  0.3711,  0.3945,\n",
       "         0.3828, -0.1367, -0.2578, -0.6719, -0.6250, -0.7617, -0.8828, -0.6094,\n",
       "         0.8164,  0.4570,  0.1133,  0.8984,  0.9141,  0.5859,  0.0312,  0.9648,\n",
       "        -0.4922, -0.2734,  0.5352,  0.0391, -0.5820,  0.1797, -0.7227,  0.9531,\n",
       "         0.0977,  0.7695, -0.3281,  0.4883, -0.4414,  0.9219, -0.8984, -0.2461,\n",
       "         0.9727, -0.6797,  0.9844,  0.8789, -0.8516, -0.3438,  0.0547,  0.8398,\n",
       "         0.3594,  0.3516,  0.6172,  0.1445,  0.4961,  0.7500,  0.4453, -0.4805,\n",
       "        -0.4688, -0.3867,  0.8828,  0.3984,  0.1055, -0.0234,  0.4375,  0.3750,\n",
       "        -0.6523,  0.0352,  0.5156, -0.2539,  0.1211, -0.7070, -0.5898,  0.3047,\n",
       "        -0.5352,  0.6016,  0.9883, -1.0000,  0.4414, -0.5781, -0.2227, -0.4258,\n",
       "         0.7422,  0.1602, -0.4219,  0.9023,  0.3633, -0.8672, -0.0781,  0.5039,\n",
       "        -0.2109,  0.0898, -0.9141,  0.8438, -0.5742,  0.7539,  0.1914,  0.6562,\n",
       "         0.3008, -0.0547, -0.5234,  0.5625, -0.2383, -0.8359,  0.7969,  0.0664,\n",
       "        -0.7891,  0.3906,  0.6719,  0.0625,  0.4609,  0.8594, -0.1094, -0.9453,\n",
       "        -0.5859, -0.3242,  0.5117,  0.4805,  0.6914, -0.1875,  0.4727, -0.1602,\n",
       "         0.2422, -0.8789, -0.0703, -0.9570, -0.4141, -0.4570, -0.6406, -0.3008,\n",
       "        -0.5508,  0.5000, -0.6992,  0.7891, -0.7539,  0.1641,  0.0820,  0.7734,\n",
       "         0.5898, -0.2266, -0.8008, -0.9492,  0.5742, -0.1211, -0.7578, -0.5117,\n",
       "         0.6289, -0.2031, -0.7188, -0.0898,  0.4922, -0.7734, -0.8711, -0.9062,\n",
       "        -0.5430, -0.1562, -0.3594, -0.0977, -0.5547,  0.2617, -0.8320,  0.2812,\n",
       "        -0.1172, -0.0273,  0.4297, -0.7266,  0.8672,  0.7852,  0.9609, -0.0195,\n",
       "        -0.1523, -0.8125,  0.5938, -0.6133, -0.3828,  0.5820, -0.9180,  0.6406,\n",
       "        -0.3203,  0.9258,  0.5586, -0.0039, -0.5391, -0.2656, -0.8242,  0.2109,\n",
       "         0.1016, -0.7383,  0.2148,  0.4258,  0.2070, -0.3398,  0.5078, -0.2930,\n",
       "         0.6680,  0.8867, -0.4297, -0.1055,  0.6094,  0.4336,  0.6211,  0.3203,\n",
       "         0.4531, -0.1133,  0.9492,  0.4141,  0.1953,  0.6953,  0.6797,  0.6328,\n",
       "         0.0938, -0.3750, -0.4961, -0.9023, -0.2969, -0.1328,  0.9102, -0.3125,\n",
       "         0.9766, -0.8906, -0.8867, -0.2500,  0.8008,  0.9570, -0.4180, -0.5156,\n",
       "        -0.6016, -0.1016,  0.1484, -0.2188,  0.7148,  0.4219, -0.1406, -0.0078,\n",
       "        -0.5586,  0.3867,  0.4492, -0.0586, -0.8086, -0.0508,  0.2461, -0.7109,\n",
       "        -0.2305])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def earth_mover_distance(input: Tensor, target: Tensor, convert_to_prob_distr=False) -> Tensor:\n",
    "    '''From: https://discuss.pytorch.org/t/implementation-of-squared-earth-movers-distance-loss-function-for-ordinal-scale/107927/2'''\n",
    "\n",
    "    # convert to probability distribution\n",
    "    if convert_to_prob_distr:\n",
    "        input = F.softmax(input, dim=-1)\n",
    "        target = F.softmax(target, dim=-1)\n",
    "\n",
    "    return torch.mean(torch.square(torch.cumsum(input, dim=-1) - torch.cumsum(target, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2488,  1.2290,  0.7107,  ...,  0.4301, -1.0904,  0.9434],\n",
      "        [-1.8892, -0.7786, -0.3803,  ..., -1.5747, -1.6249, -0.4611],\n",
      "        [ 2.6743,  1.2802,  0.8269,  ...,  0.9371,  2.3421,  0.1913],\n",
      "        ...,\n",
      "        [ 0.5909, -0.6343, -0.2968,  ...,  0.3054,  0.7932,  1.0691],\n",
      "        [-1.3597, -1.1690, -0.1044,  ...,  0.4661,  0.1656,  1.2770],\n",
      "        [ 1.5634, -1.6232,  0.4847,  ..., -0.5152,  0.7526, -0.3520]])\n",
      "tensor([[ 0.7488,  1.2290,  0.7107,  ...,  0.4301, -1.0904,  0.9434],\n",
      "        [-1.3892, -0.7786, -0.3803,  ..., -1.5747, -1.6249, -0.4611],\n",
      "        [ 3.1743,  1.2802,  0.8269,  ...,  0.9371,  2.3421,  0.1913],\n",
      "        ...,\n",
      "        [ 1.0909, -0.6343, -0.2968,  ...,  0.3054,  0.7932,  1.0691],\n",
      "        [-0.8597, -1.1690, -0.1044,  ...,  0.4661,  0.1656,  1.2770],\n",
      "        [ 2.0634, -1.6232,  0.4847,  ..., -0.5152,  0.7526, -0.3520]])\n"
     ]
    }
   ],
   "source": [
    "step1 = torch.randn(512, 1024)\n",
    "step2 = step1.clone()\n",
    "step2[:, 0] += 0.5\n",
    "\n",
    "print(step1)\n",
    "print(step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7848e-07)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earth_mover_distance(step1, step2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6995,  0.1876,  1.5932],\n",
       "        [ 1.4654,  1.3840,  0.4774]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(step1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sample = torch.load('/home/nikita/e2e-driving/sample-1658673603.5867388-9368.62598.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([512, 513])\n",
      "odd: torch.Size([256, 513])\n",
      "even: torch.Size([256, 513])\n",
      "loss: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "logits = loss_sample['logits']\n",
    "odd = loss_sample['odd']\n",
    "even = loss_sample['even']\n",
    "loss = loss_sample['loss']\n",
    "\n",
    "print('logits:', logits.shape)\n",
    "print('odd:', odd.shape)\n",
    "print('even:', even.shape)\n",
    "print('loss:', loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.7793, -0.7754,  ..., -0.3466, -0.3476, -0.3487],\n",
       "        [ 0.0000, -0.7824, -0.7784,  ..., -0.3469, -0.3480, -0.3491],\n",
       "        [ 0.0000, -0.7029, -0.6990,  ..., -0.3349, -0.3360, -0.3370],\n",
       "        ...,\n",
       "        [ 0.0000, -1.0124, -1.0089,  ..., -0.2926, -0.2936, -0.2946],\n",
       "        [ 0.0000, -1.1167, -1.1133,  ..., -0.3901, -0.3911, -0.3922],\n",
       "        [ 0.0000, -1.1110, -1.1076,  ..., -0.3902, -0.3913, -0.3924]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.7793, -0.7754,  ..., -0.3466, -0.3476, -0.3487],\n",
       "        [ 0.0000, -0.7029, -0.6990,  ..., -0.3349, -0.3360, -0.3370],\n",
       "        [ 0.0000, -1.2032, -1.1998,  ..., -0.1765, -0.1899, -0.2033],\n",
       "        ...,\n",
       "        [ 0.0000, -0.5586, -0.5366,  ..., -0.3946, -0.3956, -0.3966],\n",
       "        [ 0.0000, -1.0001, -0.9966,  ..., -0.2945, -0.2956, -0.2966],\n",
       "        [ 0.0000, -1.1167, -1.1133,  ..., -0.3901, -0.3911, -0.3922]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.7824, -0.7784,  ..., -0.3469, -0.3480, -0.3491],\n",
       "        [ 0.0000, -0.6881, -0.6842,  ..., -0.3355, -0.3365, -0.3376],\n",
       "        [ 0.0000, -1.2157, -1.2123,  ..., -0.1670, -0.1801, -0.1932],\n",
       "        ...,\n",
       "        [ 0.0000, -0.6408, -0.6185,  ..., -0.3954, -0.3965, -0.3976],\n",
       "        [ 0.0000, -1.0124, -1.0089,  ..., -0.2926, -0.2936, -0.2946],\n",
       "        [ 0.0000, -1.1110, -1.1076,  ..., -0.3902, -0.3913, -0.3924]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('e2e')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "322c5c4facabdc6bcc8c78b7959ccdee3ccc8c0dc306e4585416789305ea23e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
