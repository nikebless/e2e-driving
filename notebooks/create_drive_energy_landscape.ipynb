{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94291015",
   "metadata": {},
   "source": [
    "# Visualize energy landscape\n",
    "\n",
    "1. Pull a model from W&B\n",
    "2. Convert the model to ONNX (scripts/pt_to_onnx.py)\n",
    "3. Run this notebook\n",
    "4. Run visualization (scripts/visualize_energy.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9344e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('pythreejs')  \n",
    "pv.start_xvfb()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import onnxruntime as ort\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from dataloading.nvidia import NvidiaCropWide, Normalize, NvidiaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8a38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy(model, x, y):\n",
    "    inputs = {'x': x, 'y': y}\n",
    "    return model.run(None, inputs)[0].squeeze()\n",
    "\n",
    "def get_dataset_energy(model, dataloader, n_samples=256, bounds=[-1.5, 1.5]):\n",
    "    all_energies = []\n",
    "    progress_bar = tqdm(total=len(dataloader), smoothing=0)\n",
    "    batch_size = dataloader.batch_size\n",
    "\n",
    "    samples = np.linspace(*bounds, num=n_samples, dtype=np.float32).reshape(1, -1).repeat(batch_size, axis=0)\n",
    "    samples = np.expand_dims(samples, axis=-1)\n",
    "\n",
    "    for i, (input, _, __) in enumerate(dataloader):\n",
    "\n",
    "        if input['image'].shape[0] != batch_size:\n",
    "            print(f'Dropping batch with {input[\"image\"].shape[0]} samples')\n",
    "            continue\n",
    "\n",
    "        inputs = input['image'].cpu().numpy()\n",
    "        energy = get_energy(model, inputs, samples)\n",
    "\n",
    "        all_energies.extend(energy)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "\n",
    "    result = np.array(all_energies)\n",
    "    return result, samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f09cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ort.InferenceSession(\"/home/nikita/e2e-driving/_models/ibc_pure_last.onnx\", providers=['CUDAExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58343d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32 n_samples: 1024\n",
      "/data/Bolt/drives-nikita-thesis/2022-07-11-15-22-50_test_vahi_ibc_dfo_drop_frames: lenght=18992, filtered=0\n"
     ]
    }
   ],
   "source": [
    "root_path = Path(\"/data/Bolt/drives-nikita-thesis/\")\n",
    "valid_paths = [root_path / \"2022-07-11-15-22-50_test_vahi_ibc_dfo_drop_frames\"]\n",
    "\n",
    "_, samples = model.get_inputs()\n",
    "batch_size, n_samples, _ = samples.shape\n",
    "\n",
    "print('batch_size:', batch_size, 'n_samples:', n_samples)\n",
    "\n",
    "# TODO: try CV2 resize as done during inference\n",
    "tr = transforms.Compose([NvidiaCropWide(), Normalize()])\n",
    "inference_set = NvidiaDataset(valid_paths, transform=tr)\n",
    "\n",
    "inference_loader = torch.utils.data.DataLoader(inference_set, batch_size=batch_size, shuffle=False,\n",
    "                                         num_workers=1, pin_memory=False, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67df29c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3ea99213ce4cf5853ca53e6661db9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping batch with 16 samples\n"
     ]
    }
   ],
   "source": [
    "energy, samples = get_dataset_energy(model, inference_loader, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e328d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('energy_valley.npy', energy)\n",
    "np.save('samples.npy', samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53d9bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18976, 1024), (1024, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[energy.shape, samples.shape]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('e2e')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "322c5c4facabdc6bcc8c78b7959ccdee3ccc8c0dc306e4585416789305ea23e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
